{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, labeling, model, predict, utils, dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Model' from '/home/farzanashfaque/Project/NER/src/Model.py'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 289/47959 [00:00<00:16, 2887.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47959/47959 [00:09<00:00, 5292.68it/s]\n",
      "100%|██████████| 38367/38367 [00:00<00:00, 91760.71it/s]\n",
      "  0%|          | 6/38367 [00:00<10:46, 59.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dependency parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38367/38367 [01:41<00:00, 376.85it/s]\n",
      "  1%|          | 407/38367 [00:00<00:09, 4069.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting labeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38367/38367 [00:02<00:00, 13167.31it/s]\n",
      "100%|██████████| 38367/38367 [00:00<00:00, 259115.85it/s]\n",
      "  0%|          | 0/38367 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for labeling: 2.9276187419891357\n",
      "Relabling...\n",
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38367/38367 [01:09<00:00, 550.09it/s]\n",
      "100%|██████████| 4796/4796 [00:07<00:00, 606.81it/s]\n",
      "100%|██████████| 4796/4796 [00:08<00:00, 573.71it/s]\n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x7f2f3a0239a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x7f2f3a0239a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7f2f464a0880>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7f2f464a0880>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7f2f464a0e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7f2f464a0e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7f2f4649b4c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7f2f4649b4c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7f2f4649b130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7f2f4649b130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7f2f4649bfa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7f2f4649bfa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7f2f4649b5e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7f2f4649b5e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7f2f43613f40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7f2f43613f40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "from labeling import Label\n",
    "from model import EntityModel\n",
    "import config\n",
    "from dataset2 import preprocess_data\n",
    "from utils import prepare_data, join_subtokens, expand_entity_dict, label_encoding\n",
    "from predict import get_predictions\n",
    "from time import time\n",
    "\n",
    "\n",
    "vals_dict = preprocess_data()\n",
    "text_train = vals_dict['text_train']\n",
    "text_val = vals_dict['text_val']\n",
    "text_test = vals_dict['text_test']\n",
    "tags_train = vals_dict['tags_train']\n",
    "tags_val = vals_dict['tags_val']\n",
    "tags_test = vals_dict['tags_test']\n",
    "entity_dict = vals_dict['entity_dict']\n",
    "\n",
    "tag2idx, idx2tag = label_encoding(tags_train)\n",
    "labeler = Label(text_train, entity_dict)\n",
    "print('Starting dependency parsing...')\n",
    "dep = labeler.dependency_parse()\n",
    "a = time()\n",
    "print('Starting labeling...')\n",
    "labels = labeler.get_labels()\n",
    "b = time()\n",
    "print('Time taken for labeling: {}'.format(b-a))\n",
    "print('Relabling...')\n",
    "labels = labeler.relabel(labels, dep)\n",
    "print('Preparing data...')\n",
    "text_enc_train, mask_train, tags_enc_train = prepare_data(text_train, labels, tag2idx)\n",
    "text_enc_val, mask_val, tags_enc_val = prepare_data(text_val, tags_val, tag2idx)\n",
    "text_enc_test, mask_test, tags_enc_test = prepare_data(text_test, tags_test, tag2idx)\n",
    "\n",
    "model = EntityModel().model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      " 534/1199 [============>.................] - ETA: 3:57 - loss: 0.1590 - accuracy: 0.9572 - precision_2: 0.9710 - recall_2: 0.9505"
     ]
    }
   ],
   "source": [
    "model2 = EntityModel().model()\n",
    "history = model2.fit([text_enc_train, mask_train],\n",
    "                     tags_enc_train,\n",
    "                     batch_size=32,\n",
    "                     epochs=config.EPOCHS,\n",
    "                     validation_data=([text_enc_val, mask_val],tags_enc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199/1199 [==============================] - 476s 397ms/step - loss: 0.0245 - accuracy: 0.9931 - precision: 0.9935 - recall: 0.9927 - val_loss: 0.4979 - val_accuracy: 0.9273 - val_precision: 0.9277 - val_recall: 0.9270\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([text_enc_train, mask_train],\n",
    "                     tags_enc_train,\n",
    "                     batch_size=config.BATCH_SIZE,\n",
    "                     epochs=config.EPOCHS,\n",
    "                     validation_data=([text_enc_val, mask_val],tags_enc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38367/38367 [00:08<00:00, 4722.66it/s]\n",
      "100%|██████████| 38367/38367 [00:00<00:00, 126474.40it/s]\n"
     ]
    }
   ],
   "source": [
    "text_pred, labels_pred = get_predictions(model, text_enc_train, mask_train, idx2tag)\n",
    "entity_dict3 = utils.expand_entity_dict(entity_dict2.copy(), text_pred, labels_pred, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38367/38367 [00:07<00:00, 4917.91it/s]\n",
      "100%|██████████| 38367/38367 [00:00<00:00, 201748.23it/s]\n",
      "100%|██████████| 38367/38367 [01:21<00:00, 471.75it/s]\n"
     ]
    }
   ],
   "source": [
    "labeler = Label(text_train, entity_dict2)\n",
    "labels = labeler.get_labels()\n",
    "labels = labeler.relabel(labels, dep)\n",
    "text_enc_train, mask_train, tags_enc_train = prepare_data(text_train, labels, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prediction: 0.7719708879788717 mins\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "\n",
    "a = time()\n",
    "preds = model.predict([text_enc_val, mask_val], batch_size=32)\n",
    "b = time()\n",
    "t = (b-a)/60\n",
    "print('Time taken for prediction: {} mins'.format(t))\n",
    "preds = np.argmax(preds, axis=2)\n",
    "y = np.argmax(tags_enc_val, axis=2)\n",
    "\n",
    "preds_ = []\n",
    "y_ = []\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds_.extend(preds[i])\n",
    "    y_.extend(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    186895\n",
      "           1       0.89      0.43      0.58      6314\n",
      "           2       0.48      0.10      0.16      5753\n",
      "           3       0.00      0.00      0.00       129\n",
      "           4       0.18      0.56      0.27        57\n",
      "           5       0.47      0.19      0.27      6788\n",
      "           6       0.86      0.98      0.91     96290\n",
      "           7       0.17      0.16      0.17        62\n",
      "           8       0.04      0.02      0.03      1797\n",
      "           9       0.71      0.26      0.38      2859\n",
      "\n",
      "    accuracy                           0.93    306944\n",
      "   macro avg       0.48      0.37      0.38    306944\n",
      "weighted avg       0.92      0.93      0.92    306944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_, preds_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2, continuous\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    186895\n",
      "           1       0.53      0.43      0.47      6314\n",
      "           2       0.48      0.11      0.18      5753\n",
      "           3       0.00      0.01      0.00       129\n",
      "           4       0.19      0.58      0.28        57\n",
      "           5       0.47      0.20      0.28      6788\n",
      "           6       0.86      0.96      0.90     96290\n",
      "           7       0.17      0.18      0.17        62\n",
      "           8       0.03      0.02      0.02      1797\n",
      "           9       0.65      0.26      0.38      2859\n",
      "\n",
      "    accuracy                           0.93    306944\n",
      "   macro avg       0.44      0.37      0.37    306944\n",
      "weighted avg       0.91      0.93      0.92    306944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('iter 2, continuous')\n",
    "print(classification_report(y_, preds_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2, discrete\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    186895\n",
      "           1       0.54      0.42      0.47      6314\n",
      "           2       0.48      0.10      0.17      5753\n",
      "           3       0.00      0.00      0.00       129\n",
      "           4       0.19      0.56      0.28        57\n",
      "           5       0.48      0.19      0.27      6788\n",
      "           6       0.86      0.96      0.91     96290\n",
      "           7       0.15      0.16      0.15        62\n",
      "           8       0.03      0.02      0.03      1797\n",
      "           9       0.68      0.27      0.38      2859\n",
      "\n",
      "    accuracy                           0.93    306944\n",
      "   macro avg       0.44      0.37      0.37    306944\n",
      "weighted avg       0.91      0.93      0.92    306944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('iter 2, discrete')\n",
    "print(classification_report(y_, preds_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'tim',\n",
       " 2: 'O',\n",
       " 3: 'eve',\n",
       " 4: 'nat',\n",
       " 5: 'gpe',\n",
       " 6: 'per',\n",
       " 7: 'org',\n",
       " 8: 'art',\n",
       " 9: 'geo',\n",
       " 0: 'PAD'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8696"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels[i])):\n",
    "        lab = labels[i][j]\n",
    "        if lab!='O' and lab!='PAD':\n",
    "            c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54469"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights('../model-weights/model2.h5')\n",
    "model.save_weights('../model-weights/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hey everyone. My name is john doe. today we will learn about \"something\". Thats all.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'lemmatizer', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intj\n",
      "ROOT\n",
      "punct\n",
      "poss\n",
      "nsubj\n",
      "ROOT\n",
      "compound\n",
      "attr\n",
      "punct\n",
      "npadvmod\n",
      "nsubj\n",
      "aux\n",
      "ROOT\n",
      "prep\n",
      "punct\n",
      "pobj\n",
      "punct\n",
      "punct\n",
      "nsubj\n",
      "ROOT\n",
      "attr\n",
      "punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
